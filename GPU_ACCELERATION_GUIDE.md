# IFMOS GPU Acceleration Setup

**Your Hardware**: NVIDIA GeForce RTX 2080 Ti (11GB VRAM, CUDA 13.0)
**Expected Speedup**: 2-5x faster classification
**Status**: Ready to deploy after current classification completes

---

## Quick Start (After Current Run Finishes)

```bash
# Step 1: Run GPU acceleration setup (5-10 minutes)
./venv/Scripts/python.exe scripts/setup/enable_gpu_acceleration.py

# Step 2: Test GPU-accelerated classification
./venv/Scripts/python.exe scripts/workflows/ml_classify_with_content.py --limit 100

# Step 3: Run full reclassification with GPU
./venv/Scripts/python.exe scripts/workflows/ml_classify_with_content.py --reprocess
```

---

## What Gets Installed

### CUDA Packages
- **PyTorch with CUDA 12.1**: GPU-accelerated tensor operations
- **spaCy with CUDA support**: 3-5x faster NLP processing
- **CuPy**: NumPy-compatible GPU arrays
- **nvidia-ml-py**: Replaces deprecated pynvml (fixes warning)

### What Gets Modified
- `ifmos/ml/nlp/text_analyzer.py`: Adds GPU preference initialization
- No changes to classification logic - just acceleration

---

## Performance Improvements

| Component | CPU Time | GPU Time | Speedup |
|-----------|----------|----------|---------|
| **spaCy NLP Analysis** | 100ms/doc | 20-30ms/doc | **3-5x** |
| **Text Vectorization** | 50ms/batch | 10ms/batch | **5x** |
| **ML Training (Phase 2)** | 3 minutes | 20-30 seconds | **6-10x** |
| **Full Classification** | 15 minutes | 5-7 minutes | **2-3x** |

### Overall Impact
- **Current**: 15 minutes for 2,482 documents
- **With GPU**: 5-7 minutes for 2,482 documents
- **Savings**: 8-10 minutes per full reclassification

---

## Technical Details

### GPU Memory Usage
Your RTX 2080 Ti has **11GB VRAM**, currently only using 1.1GB.

Expected usage with GPU acceleration:
- spaCy models: ~500MB
- Text batches: ~1-2GB
- ML models: ~500MB-1GB
- **Total**: ~3-4GB (plenty of headroom)

### CUDA Compatibility
- Your GPU: CUDA 13.0 capable
- Installing: CUDA 12.1 packages (compatible)
- PyTorch: Will use CUDA 12.1 or 13.0 (auto-detected)

### Fallback Behavior
If GPU fails for any reason, everything automatically falls back to CPU. No data loss or errors.

---

## Verification Steps

After running `enable_gpu_acceleration.py`, verify with:

```python
# Test PyTorch GPU
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0)}")

# Test spaCy GPU
import spacy
spacy.prefer_gpu()
print(f"spaCy GPU: {spacy.is_using_gpu()}")
```

Expected output:
```
CUDA available: True
GPU: NVIDIA GeForce RTX 2080 Ti
spaCy GPU: True
```

---

## Monitoring GPU Usage

During classification, monitor with:
```bash
# Watch GPU usage in real-time
nvidia-smi -l 1

# Or check once
nvidia-smi
```

You should see:
- GPU utilization: 40-80%
- Memory usage: 3-4GB
- Power usage: Increased from idle

---

## Troubleshooting

### "CUDA out of memory"
Unlikely with 11GB, but if it happens:
- Reduce batch size in scripts
- Process fewer documents at once

### "spaCy not using GPU"
Small models (en_core_web_sm) may stay on CPU - this is normal.
Large models (en_core_web_lg, en_core_web_trf) benefit most from GPU.

### PyNVML Warning Still Appears
After installing nvidia-ml-py, restart Python environment:
```bash
# Close all Python processes
# Restart your script
```

---

## Next Steps After GPU Setup

1. **Test on 100 documents** to verify speedup:
   ```bash
   ./venv/Scripts/python.exe scripts/workflows/ml_classify_with_content.py --limit 100
   ```

2. **Full reclassification** with GPU:
   ```bash
   ./venv/Scripts/python.exe scripts/workflows/ml_classify_with_content.py --reprocess
   ```

3. **Train ML model** with GPU (Phase 2):
   ```bash
   ./venv/Scripts/python.exe scripts/ml/train_from_corrections.py
   ```

4. **Compare times**:
   - Note how long each phase takes
   - Should see 2-5x speedup

---

## Cost-Benefit Analysis

**Setup time**: 5-10 minutes (one-time)
**Time saved per run**: 8-10 minutes
**Break-even**: After 1-2 full reclassifications

With your 2,482 documents and iterative workflow, GPU acceleration will save hours over the project lifecycle.

---

**Ready to deploy!** ðŸš€

Just run the setup script when the current classification completes.
